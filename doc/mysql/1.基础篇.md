## 基础篇

#### 1.一条查询语句是如何执行的？

MySQL的逻辑基础架构图：

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/0d2070e8f84c4801adbfa03bda1f98d9.png)1）连接器

作用：我们会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。

```shell
mysql -h$ip -P$port -u$user -p
```

- 如果用户名或者密码不对，则会返回：”Access denied for user“
- 一旦连接成功之后，连接器会到权限表中查询所拥有的权限，之后，这个连接在的权限判断逻辑就都依赖于此时读到的权限。

建立连接的过程通常是比较复杂的，因此建议我们尽量减少创建连接的动作，即尽量使用长连接，但是长连接存在以下问题：

- 全部使用长连接之后，我们可能会发现有时候MYSQL的内存涨的非常快，这是因为MYSQL执行过程中临时使用的内存是管理在连接对象中的，这些资源在连接断开的时候才释放，因此，如果长连接累积下来，可能会导致内存占用太大，出现OOM，现象上来看就是MYSQL异常重启了。

解决方案：

- 定期断开长连接或程序中判断执行过一个占用内存的大查询之后，断开连接，之后查询再重连。
- MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚创建完成的状态。

2）查询缓存

作用：之前执行过的语句以及结果会以key-value的形式，被直接缓存到内存中，key为查询语句，value为查询结果，如果我们的查询可以在缓存中找到，会进行权限验证，一旦通过那么就会将value就会直接返回给客户端。

建议：**大多数形况下建议不要使用查询缓存。**查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

关闭查询缓存：query_cache_type=DEMAND

显示使用查询缓存：select SQL_CACHE * from T where ID=10；

**注意**：MySQL 8.0 版本直接将查询缓存的整块功能删掉了

3）分析器：识别出我们要做什么

- 词法分析：识别出我们输入的SQL语句中的字符串代表什么。例如我们输入的select语句，那么他就是一个查询语句，识别出字段，表的别名等。
- 语法分析：按照语法规则，判断我们输入的MYSQL语句是否满足语法。如果语句不对则会返回”You have an error in your SQL syntax“，那么我们应该关注”user near“的内容。

4）优化器：怎么做

优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。优化器阶段完成后，这个语句的执行方案就确定下来了。可以使用explain来查看某个SQL的执行计划。

5）执行器

作用：执行优化后的SQL语句，获取结果。执行过程如下所示。

1. 判断一下你对这个表 T 有没有执行查询的权限，如果没有则返回权限不足。在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限
2. 如果有全权限则打开表，根据表的引擎定义，去使用引擎提供的接口：例如表ID字段没有索引，那么就会执行如下过程：
   1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；
   2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
   3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

慢查询日志中的字段`rows_examined`表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行(例如回表现象)，因此引擎扫描行数跟 rows_examined 并不是完全相同的。

**问题**：如果我们查询了不存在的列，那么Unknown column ‘k’ in ‘where clause’”这个错误是在哪一个阶段爆出来？

分析器阶段抛出的错误，会检查表名，列名，函数签名等是否合法。

#### 2.一条更新语句的执行流程是什么？

和之前的查询过程基本一致，但是更新流程还涉及到了两个重要的日志模块：`redo log`（重做日志）和 `binlog`（归档日志）

1）redo log：是 InnoDB 引擎特有的日志，属于引擎层log

WAL(Write-Ahead logging)：先写日志(redo log)，再写磁盘(数据页落盘)。

具体来说：当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。

如果更新的数据特别多怎么办呢？

放下手中的工作(停止更新一段时间)将一部分数据更新到磁盘中，然后清除这部分redo log，这样就有了一部分空间。

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/16a7950217b3f0f4ed02db5db59562a7.png)

- write pos 是当前记录的位置，循环写的过程，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。
- checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

- write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

有了 redo log(也存在)，InnoDB 就可以保证即使数据库发生异常重启，之前**提交**的记录都不会丢失，这个能力称为 crash-safe。

2）binlog日志：Server层的日志，也称为归档日志

binlog两种模式：一般采用row，但是日志会比较大。

- statement：记录SQL语句。
- row：记录行的内容，更新以前以及更新以后。

为什么要有两份日志？

因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。

两者主要有三点不同：

- redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
- redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
- redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

执行器和InnoDB引擎在执行update语句的时候的内部流程：

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

图解：浅色表示在InnoDb中执行；深色框表示在执行器中执行的。

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/2e5bff4910ec189fe1ee6e2ecc7b4bbe.png)

redo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。

**两阶段提交**：redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。

反证法说明为什么需要两阶段提交：

- 先写 redo log 后写 binlog：假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。
- 先写 binlog 后写 redo log：如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。

建议：

- redo log 用于保证 crash-safe 能力。`innodb_flush_log_at_trx_commit` 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。
- sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

问题：

1）什么场景下一天一备比一周一备更具有优势？或者说它影响了数据库系统的哪个指标？

一天一备如果系统发生crash现象，那么恢复的时候就会快一些，备份库的数据更加接近crash时的状态，binlog日志相对更少，恢复起来比较快。但是频繁的全量备份需要消耗更多的存储空间。

#### 3.事务隔离

> 事务就是要保证一组数据库操作，要么全部成功，要么全部失败。MYSQL事务的支持是在引擎层实现的。

1）基本概念

当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了**隔离级别**的概念。

在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。

- 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。
- 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

```sql
mysql> create table T(c int) engine=InnoDB;
insert into T(c) values(1);
```

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/7dea45932a6b722eb069d2264d0066f8.png)

我们来看看在不同的隔离级别下，事务 A 会有哪些不同的返回结果，也就是图里面 V1、V2、V3 的返回值分别是什么。

- 若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。
- 若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。
- 若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。
- 若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。

在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。

- 在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。
- 在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；
- 而“串行化”隔离级别下直接用加锁的方式来避免并行访问同一数据。

Oracle 数据库的默认隔离级别其实就是“读提交”，因此如果数据库从oracle到mysql时要进行设置。

```sql
--查看事务隔离级别
show variables like 'transaction_isolation';
```

2）事务隔离的实现--MVCC，数据库的多版本的并发控制

> 以可重复读为例。

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设将一个值1按照顺序改为2，3，4，那么在回滚日志里就会存在类似下面的记录。

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/d9c313809e5ac148fc39feff532f0fee.png)

当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。

什么时候删除回滚日志？

系统会判断，当没有事务再需要用到这些回滚日志时，也就是当前系统里没有比这个回滚日志更早的read-view的时候，回滚日志会被删除。

注：在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。

建议：尽量不要使用长事务

- 会导致系统中存在很老的事务视图，在这个事务提交之前，回滚记录都需要保存，会占用大量的存储空间。
- 长事务还占用锁资源，也可能拖垮整个库

```sql
--查询长事务
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
```

3）事务的启动方式

Mysql的事务启动方式有以下几种

- 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。
- set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。而有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令，这就导致接下来的查询都在事务中，如果是长连接，就意外导致了长事务。



问题：

你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

应用开发的角度：

- 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。
- 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。
- 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）

数据库端来看：

- 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / kill；
- Percona 的 pt-kill 这个工具不错，推荐使用；
- 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；
- 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。

#### 4.索引

> 索引就像书的目录一样，索引的出现其实就是为了提高数据查询的效率。

1）常见的索引模型

哈希表：类似于java中的HashMap，只适合于等值查询的的场景，比如Memcached以及一些NoSQL引擎。

有序数组：等值查询和范围查询场景中的性能非常优秀(id底层存储)，可以使用二分法查询，事件复杂度O(logn)。但是当我们向中间插入数据的时候，就比较麻烦，因为需要进行数据挪动。因此有序数组索引只适用于静态存储引擎。

搜索树：以二叉树搜索树为例，查找一个元素的时间复杂度是O(logn)。

- 当数据量大的时候，比如一颗100w节点的平衡二树，树的高度为20，查询一个数据可能需要访20个数据块，查找效率就会降低。因此一般数据库引擎会采用多叉树来降低树的高度以减少查询的数据块的数量来提高查询性能。
- 当我们需要范围查找的时候，需要向父节点查找，因此范围查找的效率也不高，因此Mysql 的Innodb也对N叉树进行了改造，将所有的数据都在叶子节点存储一份，并使用链表相连，即B+树。

2）InnoDb的索引模型

InnoDb使用B+ tree索引模型，假设我们有一个表的DDL语句如下所示。

```sql
mysql> create table T(
	id int primary key, 
	k int not null, 
	name varchar(16),
index (k))engine=InnoDB;
```

表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/dcda101051f28502bd5c4402b292e38d.png)

根据叶子节点的内容，索引类型分为`主键索引`和`非主键索引`。

- 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。
- 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

那么基于主键索引和普通索引的查询有什么区别？

- 如果依据主键查询的方式，则只需要搜索ID这棵B+树。
- 如果是普通索引进行查询，则需要先搜索K索引树，得到ID的值为500，再到ID索引树中搜索一次，这个过程称之为回表。

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

3）索引维护

B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。

- 新增数据的可能导致数据页内容超过可存储范围，需要进行**页分裂**；

- 删除数据可能会导致数据页的内容小于最小存储容量，需要进行**页合并**；

4）自增主键的适用场景

自增主键的优点：

- 性能：因为自增主键是插入数据模式，正好符合递增插入的场景，不涉及数据的挪动，也不会触发叶子节点的分裂。如果以业务字段作为主键，往往不容易保证有序插入，这样写数据成本相对比较高。
- 存储：假设业务字段是身份证号，那么长度就比较长，占用空间比较大，而二级索引的叶子节点存储逐渐的值，这样每个叶子节点占用的磁盘空间会比较大。显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。

什么场景下适合直接使用业务字段做主键呢？K-V场景

- 只有一个主键索引存在。
- 该索引必须是唯一索引。

这样就不需要考虑二级索引存储主键占用的存储，还可以避免回表查找，可以提高查询效率。

5）回表查找的问题

如下所考虑的模型中，如果我们要执行：select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/dcda101051f28502bd5c4402b292e38d.png)

1. 查询k树，k=3获得主键id=300。
2. 查询Id树，获取id=500的数据行R3。
3. 查询k树，k=5获得主键id=500。
4. 查询Id树，获取id=500的数据行R4。
5. 查询k树，k=6，不满足要求，退出查询。



在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。因此我们有没有办法去避免回表呢？

覆盖索引：

如果我们要执行的语句改为select ID from T where k between 3 and 5，这个时候ID在k树已经存在了，因此可以直接提供查询结果，不需要回表，也就是说，索引K已经覆盖了我们的查询需求，我们称之为覆盖索引。因此覆盖索引也是一个常用的性能优化手段。

注意：在引擎内部使用覆盖索引在索引 k 上其实读了三个记录，R3~R5（对应的索引 k 上的记录项），但是对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2。

最左前缀原则：

B + 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。假设我们使用(name,age)这个联合索引来分析。

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/89f74c631110cfbc83298ef27dcd6370.jpg)

当我们使用like '张%'来查找的时候，这个时候就可以用上这个索引，查找到第一个符合记录的是ID3，然后向后遍历，直到不满足条件为止。只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。

基于以上问题：当我们建立联合索引的时候，需要考虑好索引内的字段顺序。当我们已经有了(a,b)这个联合索引后，一般就不需要单独的在a上建立索引了，因此第一原则是：如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。

如果查询条件中只有b的语句，那么联合索引(a,b)是无法使用，此时我们需要维护另外一个索引b。

索引下推：MySQL 5.6引入

假如我们存在一个联合索引(name,age)，我们需要查询出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”，那么SQL语句就如下所示：

```sql
mysql> select * from tuser where name like '张%' and age=10 and ismale=1;
```

利用最左前缀规则，那么查询联合索引的时候就可以先定位到name以张开头的记录，如果5.6以前就需要循环回表查找了；5.6的索引下推，可以对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表的次数。

- 5.6以前：

  ![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/b32aa8b1f75611e0759e52f5915539ac.jpg)

- 5.6以后：

  ![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/76e385f3df5a694cc4238c7b65acfe1b.jpg)







问题：

如下方式重建索引有什么问题？索引因为页分裂/页合并的原因，可能导致数据页又空洞，重建索引可以使得页面的利用率变高，索引更加紧凑，节省空间。

```sql
-- 重建非主键索引：合理
alter table T drop index k;
alter table T add index(k);

-- 重建主键索引：不可理，无论是删除主键还是创建主键，都会将整个表重建，如果一块执行就相当于第一个语句白做了，可以使用alter table T engine=InnoDB，会自动对主键创建索引。
alter table T drop primary key;
alter table T add primary key(id);
```

#### 5 锁

>数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。

1）全局锁

全局锁就是对整个数据库实例加锁，让整个库处于一种只读状态，其他现成的更新语句就会被阻塞。MySQL提供了一个加全局读锁的方法，如下所示：

```sql
Flush tables with read lock (FTWRL)
```

典型的应用场景：全库逻辑备份，也就是将整个库每个表都select出来存储为文本。听上去会存在如下危险：

- 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；
- 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。

如果不加锁就会导致备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。但是我们也可以在可重复读隔离级别下开启一个事务，这样在事务提交之前，逻辑视图是不会发生变化的。

官方自带的逻辑备份工具是 `mysqldump`。当 mysqldump 使用参数`–single-transaction` 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。但是使用这个功能的前提是`引擎支持这个隔离级别`。对于MyIAM这种不支持事务的引擎，那么就只能使用全局锁了。



全局锁和set global readonly=true的方式有什么区别呢？

- 在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。
- 在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

2）表级锁

MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

表锁的语法是 lock tables … read/write。

```sql
lock tables t1 read, t2 write;
```

注：lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。对于InnoDb这种支持行锁的引擎，一般不使用lock tables命令来控制并发。

MDL（metadata lock)：MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。因此在MySQL 5.5版本加入了MDL，当对一个表做增删改查操作的时候，加MDL读锁，当对表做结构变更操作的时候，加MDL写锁。

- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

给表加入字段/修改数据/加索引，需要扫描全表数据，在对需要频繁查询的表操作的时候，我们需要特别的小心，以免对线上服务造成影响。

假设有如下操作：事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/7cf6a3bf90d72d1f0fc156ececdfb0ce.jpg)

1. session A会对表t加MDL读锁。
2. session B需要t表MDL读锁，所以可以正常执行。
3. session C 需要MDL写锁，因为读锁没释放，所以C被blocked。
4. Session D要在t上申请MDL读锁，却会被session C阻塞。相当于整个表不可读写。
5. 如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。

那么我们应该如何安全的给小表加字段呢？

- 首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。

如果实在是需要变更一个热点表，我们可以在alter table的时候设定等待时间，如果等待时间内拿到写锁那么就更改，如果拿不到就不要阻塞后面的业务语句。我们可以多次尝试这个过程。

```sql
ALTER TABLE tbl_name NOWAIT add column ...
ALTER TABLE tbl_name WAIT N add column ... 
```

问题：

备份一般都会在备库上执行，你在用–single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？



3）行锁

> MySQL 的行锁是在引擎层由各个引擎自己实现的。比如MyISAM就不支持行锁。

在下面的操作序列中，事务 B 的 update 语句执行时会是什么现象呢？假设字段 id 是表 t 的主键。

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/51f501f718e420244b0a2ec2ce858710.jpg)

事务A会将id=1和id=2的数据行锁住，只有当事务提交之后，事务B才可以继续执行。所以k的最终值(1,k+3),(2,k+1)。在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是`两阶段锁协议`。

优化点：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。如何安排SQL的顺序呢？

1. 从顾客 A 账户余额中扣除电影票价；update
2. 给影院 B 的账户余额增加这张电影票价；update
3. 记录一条交易日志；insert

根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。

4）死锁和死锁检测

> CPU 消耗接近 100%，但整个数据库每秒就执行不到 100 个事务。

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。如下图所示就是数据库死锁现象：

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/4d0eeec7b136371b79248a0aed005a52.jpg)

解决策略：

- 直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。默认为50s。当出现死锁之后，第一个被锁住的线程要过50s才会超时退出，其他线程才可以继续执行，对于在线服务来说，这个等待时间是无法接受的，但是太短又会出现误伤。所以**不推荐**。
- 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。但是又额外的负担
  - 执行过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁，但是最终没有出现死锁现象。
  - 时间复杂度：这是一个O(n)级别的操作n为锁住同一行的线程数，如果1000个并发线程同时更新同一行，那么死锁检测的量级就是100w这个界别。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。
  - 如何解决热点行更新导致的性能问题呢？
    - 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是一旦出现死锁，那么就会出现大量的超时，业务有损。**不推荐**。
    - 另一个思路是控制并发度，在服务器端进行控制(中间件或修改源码)，在修改相同行的时候，进入引擎之前进行排队，这样就不会出现大量死锁检测工作。
    - 考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。但是需要根据业务逻辑进行详细的设计。

问题：

如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：

1. 直接执行 delete from T limit 10000;--会锁住前10000行，时间比较长，如果又其他的查询会被阻塞，而且大事务还会导致主从延迟。
2. 在一个连接中循环执行 20 次 delete from T limit 500;--没有commit，所以消耗时间也比较久。
3. 在 20 个连接中同时执行 delete from T limit 500；--会出现锁竞争。

#### 6 普通索引和唯一索引如何选择

> 从查询和更新语句的性能影响来进行分析。

1）查询过程

InnoDB的数据是按照数据页为的那位来读写的，也就是说，当需要一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为的那位，将其整体读入到内存中，默认一个数据页的大小为16Kb，而一个数据页如果用来存储Long类型，那么可以存储将近2000个key。所以对于普通索引来说，要多做的那一次“查找和判断下一跳记录”的操作，就只需要一次指针寻找和一次计算，因此普通索引和唯一索引的性能差异是很小的。

2）更新过程

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性

注：change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。

将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

什么条件下会使用Change Buffer?

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。**所以只有普通索引可以使用change Buffer**。

change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。

因此插入一个新纪录(4,400)的话，InnoDB处理流程如下：

- 要更新的记录的数据页在内存中：只是一个判断，基本没有性能差别
  - 唯一索引：找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；
  - 普通索引：找到 3 和 5 之间的位置，插入这个值，语句执行结束。
- 要更新的记录的数据页不在内存中：将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。
  - 唯一索引：将数据页读入内存，判断没有冲突，插入这个值，语句执行结束；
  - 普通索引：则是将更新记录在 change buffer，语句执行就结束了。

Change Buffer的使用场景：写多读少的场景

- 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。
- 假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。

如何选择？

- 这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。
- 如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。
- 特别地，在使用机械硬盘(随机读写速度比较慢)时，change buffer 这个机制的收效是非常显著的。

change buffer 和 redo log：假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。

分析这条更新语句，你会发现它涉及了四个部分：内存、redo log（ib_log_fileX）、 数据表空间（t.ibd）、系统表空间（ibdata1）。

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/980a2b786f0ea7adabef2e64fb4c4ca3.png)

执行了如下的操作：写了两次内存，一次磁盘(顺序读写，WAL)

1. Page 1 在内存中，直接更新内存；
2. Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息
3. 将上述两个动作记入 redo log 中（图中 3 和 4）。

那么在此之后的读请求，需要如何处理呢？

比如，我们现在要执行 select * from t where k in (k1, k2)。这里，我画了这两个读请求的流程图。

![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/6dc743577af1dbcbb8550bddbfc5f98e.png)

从图中可知：

- 读 Page 1 的时候，直接从内存返回。有几位同学在前面文章的评论中问到，WAL 之后如果读数据，是不是一定要读盘，是不是一定要从 redo log 里面把数据更新以后才可以返回？其实是不用的。你可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。
- 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。

结论：**redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。**

#### 7 Mysql为什么有的时候会选错索引？

优化器的选择策略：

- 扫描行数，但会计算回表消耗：当遇到explain的结果预估的rows值和实际情况相差比较大，我们可以采用analyze table来让其重新进行扫描。

索引选择异常和处理：

- 使用`force index`强行选择一个索引：**不优美，及时性也不高**，往往只有线上出问题，分析之后才会使用这个方式
- 数据库层进行处理：我们可以考虑修改语句，引导 MySQL 使用我们期望的索引。**不通用**
- 在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。

#### 8 如何给字符串字段加索引？

> 现在几乎所有的系统都支持邮箱登录，如何在邮箱这样的字符字段上建立合理的索引？

1）完整索引

直接创建完整索引，这样比较占用空间。

2）前缀索引

顾名思义，以某个字段的前n个字节当作索引。

```sql
alter table SUser add index index2(email(6));
```

优点：

- 索引占用空间会比完全的存储字符串来说要少。

缺点：

- 可能会因为索引的区分度的原因而增加额外的记录扫描次数。
- 因为系统不确定前缀索引的定义是否包含了所有的系统信息，因此会丢失覆盖索引优化。

因此想要使用好前缀索引，那么我们一定要定义好长度，有以下的小技巧：通过统计索引上有多少个不同的值来判断要使用多长的前缀。

1. 定义好可以接受的前缀索引损失的区分度，比如5%。

2. 使用以下sql来判断选取多长的长度比较合适。

   ```sql
   mysql> select 
     count(distinct left(email,4)）as L4,
     count(distinct left(email,5)）as L5,
     count(distinct left(email,6)）as L6,
     count(distinct left(email,7)）as L7,
   from SUser;
   ```

3）倒叙索引

因为前缀的区分度不高，所以也可以尝试将使用`倒叙+前缀索引`

4）使用hash手段

1. 可以在表中再建立一个证书字段，来保证身份证的校验码，同时创建索引。
2. 然后每次插入新记录的时候，都同时用 crc32() 这个函数得到校验码填到这个新字段。由于校验码可能存在冲突，也就是说两个不同的身份证号通过 crc32() 函数得到的结果可能是相同的，所以你的查询语句 where 部分要判断 id_card 的值是否精确相同。这样索引的长度就变成4个字节，比起原来小很多。

倒叙和hash存储的异同点：

- 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。
- 在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。
- 从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数

#### 9 为什么MYSQL会"抖"一下？

> 一条SQL语句正常执行的时候特别快，但是偶尔几次会特别慢，这样的常见很难复现且持续时间很短。

WAL机制影响刷脏页操作和执行时机。利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能，但是页存在内存脏页现象，引出flush操作。

1）脏页和干净页

内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为`脏页`。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为`干净页`。

2）flush操作

平时执行很快的更新操作，其实就是在写内存和日志，而 MySQL 偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）。那么什么时候会引发数据库的flush过程呢？什么过程下会将redo log的记录写到数据页呢？

- redo log写满了：这个时候，系统会停止所有的更新操作，flush一部分redo log的内容到数据页中。
- 系统内存不足：当需要新的内存页发现内存不够用了，就需要淘汰一些数据页，如果淘汰的是脏页，那么就要先将脏页写到磁盘。
- MYSQL因为系统空闲的时候：可能会flush数据，即便是很忙的时候，也需要见缝插针的flush数据。
- MYSQL正常关闭的时候：会将内存的脏页都flush到磁盘上，这样MYSQL启动的时候，就可以直接从磁盘上读数据，启动系统很快。

第一种需要尽量避免的，因为系统直接进入不可更新状态。

第二种是内存不够了，要写脏页到磁盘，是常态，InnoDB使用缓冲池(buffer pool)管理内存，缓冲池中的内存页有三种状态：

- 还没使用。
- 使用了，但是是干净的(没有更新或者已经flush)。
- 使用了，并且是脏页(更新了或者还没有flush)

InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。

尽管刷脏页是常态，但是如果出现以下两种情况，都会明显影响性能，因为InnoDB也会控制脏页比例

- 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
- 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。

3）InnoDB刷脏页的控制策略

1. 正确的告诉MYSQL服务器的IO能力，这样InnoDB才知道全力刷脏页的时候有多块。设置innodb_io_capacity。建议设置为磁盘的IOPS。可以使用fio来测试，命令如下：

   ```shell
   fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest 
   ```

2. 计算刷脏页的速度：`innodb_max_dirty_pages_pct`是脏页比例上限，默认为75%，InnoDB会根据当前的脏页比例(假设为M)，计算出一个0到100之间的数字，伪代码如下所示：

   ```java
   F1(M)
   {
     if M>=innodb_max_dirty_pages_pct then
         return 100;
     return 100*M/innodb_max_dirty_pages_pct;
   }
   ```

   InnoDB每次写入的日志都有一个序号，当前写入的序号和checkpoint对应的序号之间的差值，假设为N，函数f2(N)，如果N越大则结果越大。

   InnoDB回旋曲f1(M)，f2(N)两个值中的较大值R，之后引擎就可以按照我们定义的全力刷脏页的能力*R%来控制刷脏页的速度。示意图如下所示：

   ![img](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/02high/cc44c1d080141aa50df6a91067475374.png)

脏页比例：Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 不要让这个值接近75%

```sql

mysql> select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty';
select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total';
select @a/@b;
```

4）连坐策略

一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。

在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。

而如果使用的是 SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。因为这时候 IOPS 往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。

在Mysql 8.0中，innodb_flush_neighbors 参数的默认值已经是 0 了。

